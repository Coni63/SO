{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python36\\envs\\machine_learning\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from gym_2048.envs.game2048_env import Game2048Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# env = Game2048Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obs = env.reset()\n",
    "# print(obs)\n",
    "# print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# state = preprocess_observation(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obs, reward, done, info = env.step(0)\n",
    "# env.render()\n",
    "# print(reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.buf = np.empty(shape=maxlen, dtype=np.object)\n",
    "        self.index = 0\n",
    "        self.length = 0\n",
    "        \n",
    "    def append(self, data):\n",
    "        self.buf[self.index] = data\n",
    "        self.length = min(self.length + 1, self.maxlen)\n",
    "        self.index = (self.index + 1) % self.maxlen\n",
    "    \n",
    "    def sample(self, batch_size, with_replacement=True):\n",
    "        if with_replacement:\n",
    "            indices = np.random.randint(self.length, size=batch_size) # faster\n",
    "        else:\n",
    "            indices = np.random.permutation(self.length)[:batch_size]\n",
    "        return self.buf[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stats:\n",
    "    def __init__(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.score = np.zeros(shape=maxlen, dtype=np.int32)\n",
    "        self.highest = np.zeros(shape=maxlen, dtype=np.int32)\n",
    "        self.index = 0\n",
    "        self.total_game = 0\n",
    "        self.reach2048 = 0\n",
    "        self.highest_reached = 0\n",
    "        \n",
    "    def append(self, score, highest):\n",
    "        self.score[self.index] = score\n",
    "        self.highest[self.index] = highest\n",
    "        self.index = (self.index + 1) % self.maxlen\n",
    "        self.total_game += 1\n",
    "        self.reach2048 += 1 if highest == 2048 else 0\n",
    "        self.highest_reached = max(self.highest_reached, highest)\n",
    "        \n",
    "    def getStat(self):\n",
    "        return [int(self.score.mean()), int(self.highest.mean()), self.total_game, self.reach2048, self.highest_reached]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def q_network(X_state, name):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        hidden1 = tf.layers.dense(inputs = X_state, \n",
    "                                 units = n_hidden,\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "        hidden2 = tf.layers.dense(inputs = hidden1, \n",
    "                                 units = n_hidden,\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "        hidden3 = tf.layers.dense(inputs = hidden2, \n",
    "                                 units = n_hidden,\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "        hidden4 = tf.layers.dense(inputs = hidden3, \n",
    "                                 units = n_hidden,\n",
    "                                 activation=tf.nn.relu,\n",
    "                                 kernel_initializer=initializer)\n",
    "        outputs = tf.layers.dense(inputs = hidden4, \n",
    "                                  units = n_outputs,\n",
    "                                  kernel_initializer=initializer)\n",
    "    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                       scope=scope.name)\n",
    "    trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "                              for var in trainable_vars}\n",
    "    return outputs, trainable_vars_by_name\n",
    "\n",
    "# def q_network(X_state, name):\n",
    "#     with tf.variable_scope(name) as scope:\n",
    "#         hidden1 = tf.layers.conv2d(X_state, filters=32, \n",
    "#                                    kernel_size=(2, 2), strides=(1, 1), padding=\"SAME\", \n",
    "#                                    activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "#         hidden2 = tf.layers.conv2d(hidden1, filters=32, \n",
    "#                                    kernel_size=(2, 2), strides=(1, 1), padding=\"SAME\", \n",
    "#                                    activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "#         hidden3 = tf.layers.conv2d(hidden2, filters=32, \n",
    "#                                    kernel_size=(2, 2), strides=(1, 1), padding=\"SAME\", \n",
    "#                                    activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "#         last_conv_layer_flat = tf.reshape(hidden3, shape=[-1, 4*4*32])\n",
    "#         dense1 = tf.layers.dense(inputs = last_conv_layer_flat, \n",
    "#                                   units = 100,\n",
    "#                                   kernel_initializer=initializer)\n",
    "#         outputs = tf.layers.dense(inputs = dense1, \n",
    "#                                   units = n_outputs,\n",
    "#                                   kernel_initializer=initializer)\n",
    "#     trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "#                                        scope=scope.name)\n",
    "#     trainable_vars_by_name = {var.name[len(scope.name):]: var\n",
    "#                               for var in trainable_vars}\n",
    "#     return outputs, trainable_vars_by_name\n",
    "\n",
    "def sample_memories(batch_size):\n",
    "    cols = [[], [], [], [], []] # state, action, reward, next_state, continue\n",
    "    for memory in replay_memory.sample(batch_size):\n",
    "        for col, value in zip(cols, memory):\n",
    "            col.append(value)\n",
    "    cols = [np.array(col) for col in cols]\n",
    "    return cols[0], cols[1].reshape(-1, 1), cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)\n",
    "\n",
    "def epsilon_greedy(q_values, step):\n",
    "    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs) # random action\n",
    "    else:\n",
    "        return np.argmax(q_values) # optimal action\n",
    "\n",
    "# def preprocess_observation(obs):\n",
    "#     return obs/2048\n",
    "\n",
    "def preprocess_observation(obs):\n",
    "    result = np.zeros((4, 4, 16), dtype = np.uint8)\n",
    "    for i, value in enumerate(obs):\n",
    "        if value != 0:\n",
    "            depth = int(math.log(value, 2)-1)\n",
    "            result[i%4, i//4, depth] = 1\n",
    "    return result.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# X = range(0, 400000, 1000)\n",
    "# y = [max(eps_min, eps_max - (eps_max-eps_min) * x/eps_decay_steps) for x in X]\n",
    "\n",
    "# plt.plot(X, y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_memory_size = 50000\n",
    "replay_memory = ReplayMemory(replay_memory_size)\n",
    "stat = Stats(10)\n",
    "\n",
    "n_input = [None, 256]\n",
    "# n_input = [None, 4, 4, 16]\n",
    "n_hidden = 100\n",
    "n_outputs = 4\n",
    "\n",
    "eps_min = 0.1\n",
    "eps_max = 1.0\n",
    "eps_decay_steps = 200000\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.95\n",
    "\n",
    "n_steps = eps_decay_steps*2  # total number of training steps\n",
    "training_start = replay_memory_size  # start training after 10,000 game iterations\n",
    "training_interval = 4  # run a training step every 4 game iterations\n",
    "save_steps = 1000  # save the model every 1,000 training steps\n",
    "copy_steps = 3000  # copy online DQN to target DQN every 10,000 training steps\n",
    "discount_rate = 0.95\n",
    "batch_size = 50\n",
    "iteration = 0  # game iterations\n",
    "checkpoint_path = \"F:/training_data/DQN/Pacman/my_dqn_2048.ckpt\" #\"./my_dqn.ckpt\"\n",
    "done = True # env needs to be reset\n",
    "\n",
    "loss_val = np.infty\n",
    "game_length = 0\n",
    "total_max_q = 0\n",
    "mean_max_q = 0.0\n",
    "\n",
    "total_reward = []\n",
    "max_reward = 0\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "X_state = tf.placeholder(tf.float32, shape=n_input)\n",
    "\n",
    "online_q_values, online_vars = q_network(X_state, name=\"q_networks/online\")\n",
    "target_q_values, target_vars = q_network(X_state, name=\"q_networks/target\")\n",
    "\n",
    "copy_ops = [target_var.assign(online_vars[var_name])\n",
    "            for var_name, target_var in target_vars.items()]\n",
    "copy_online_to_target = tf.group(*copy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"train\"):\n",
    "    X_action = tf.placeholder(tf.int32, shape=[None, 1])  # action taken (shape batch x 1)\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1])       # Q value computed (shape batch x 1)\n",
    "    q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs), axis=1, keepdims=True)  # element-wise product Q-value x Action_OHE(batch x 4)\n",
    "    error = tf.abs(y - q_value)\n",
    "    clipped_error = tf.clip_by_value(error, 0.0, 1.0)\n",
    "    linear_error = 2 * (error - clipped_error)\n",
    "    loss = tf.reduce_mean(tf.square(clipped_error) + linear_error)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter 121495\t256(256,)\n",
      " Iter 121579\t256(256,)\n",
      " Iter 121721\t256(256,)\n",
      " Iter 121834\t256(256,)\n",
      " Iter 121958\t256(256,)\n",
      " Iter 122169\t256(256,)\n",
      " Iter 122345\t256(256,)\n",
      " Iter 122436\t256(256,)\n",
      " Iter 122517\t256(256,)\n",
      " Iter 122646\t256(256,)\n",
      " Iter 122750\t256(256,)\n",
      " Iter 122878\t256(256,)\n",
      " Iter 122950\t256(256,)\n",
      " Iter 123033\t256(256,)\n",
      " Iter 123169\t256(256,)\n",
      " Iter 123292\t256(256,)\n",
      " Iter 123366\t256"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0e20a923cb3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m#         print(\"\"\"\\r Iter {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:2f}\\tMean Max-Q {:2f}\\t AVG Score {}\\tAVG Highest {}\\t Game {} \\t Win {}\\t Best {} \\t\"\"\".format(iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q, avg_score, avg_highest, count_game, count_success, highest_reached), end=\"\\r\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m#         print(\"Iter {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:2f}\\tMean Max-Q {:2f}\".format(iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\\r\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r Iter {}\\t{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhighest_reached\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# game over, start again\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mevent_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send (zmq\\backend\\cython\\socket.c:7305)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send (zmq\\backend\\cython\\socket.c:7048)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy (zmq\\backend\\cython\\socket.c:2920)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq\\backend\\cython\\socket.c:9621)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Game2048Env()\n",
    "\n",
    "with tf.Session(config = config) as sess:  #config = config\n",
    "#     if os.path.isfile(checkpoint_path + \".index\"):\n",
    "#         saver.restore(sess, checkpoint_path)\n",
    "#     else:\n",
    "#         init.run()\n",
    "#         copy_online_to_target.run() \n",
    "    init.run()\n",
    "    copy_online_to_target.run()\n",
    "    \n",
    "    while True:\n",
    "        step = global_step.eval()\n",
    "        if step >= n_steps:\n",
    "            break\n",
    "        iteration += 1\n",
    "        avg_score, avg_highest, count_game, count_success, highest_reached = stat.getStat()\n",
    "#         print(\"\"\"\\r Iter {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:2f}\\tMean Max-Q {:2f}\\t AVG Score {}\\tAVG Highest {}\\t Game {} \\t Win {}\\t Best {} \\t\"\"\".format(iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q, avg_score, avg_highest, count_game, count_success, highest_reached), end=\"\\r\")\n",
    "#         print(\"Iter {}\\tTraining step {}/{} ({:.1f})%\\tLoss {:2f}\\tMean Max-Q {:2f}\".format(iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q), end=\"\\r\")\n",
    "        print(\"\\r Iter {}\\t{}\".format(iteration, highest_reached), end=\"\")\n",
    "    \n",
    "        if done: # game over, start again\n",
    "            obs = env.reset()\n",
    "            state = preprocess_observation(obs)\n",
    "            print(state.shape)\n",
    "\n",
    "        # Online DQN evaluates what to do\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "\n",
    "        # Online DQN plays\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_observation(obs)\n",
    "\n",
    "        # Let's memorize what happened\n",
    "        replay_memory.append((state, action, reward, next_state, 1.0 - done))\n",
    "        state = next_state\n",
    "\n",
    "        # Compute statistics for tracking progress (not shown in the book)\n",
    "        total_max_q += q_values.max()\n",
    "        game_length += 1\n",
    "        if done:\n",
    "            mean_max_q = total_max_q / game_length\n",
    "            total_max_q = 0.0\n",
    "            game_length = 0\n",
    "            stat.append(env.score, env.highest())\n",
    "\n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue # oly tranin after warmup period and at regular intervals\n",
    "        \n",
    "        # Sample memories and use the target DQN to produce the target Q-Value\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = sample_memories(batch_size)\n",
    "        next_q_values = target_q_values.eval(feed_dict={X_state: X_next_state_val})\n",
    "        y_val = rewards + continues * discount_rate * np.max(next_q_values, axis=1, keepdims=True)\n",
    "\n",
    "        # Train the online DQN\n",
    "        _, loss_val = sess.run([training_op, loss], feed_dict={\n",
    "            X_state: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "        # Regularly copy the online DQN to the target DQN\n",
    "        if step % copy_steps == 0:\n",
    "            copy_online_to_target.run()\n",
    "\n",
    "#         # And save regularly\n",
    "#         if step % save_steps == 0:\n",
    "#             saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256,)\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value q_networks/online/dense_1/kernel\n\t [[Node: q_networks/online/dense_1/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@q_networks/online/dense_1/kernel\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](q_networks/online/dense_1/kernel)]]\n\nCaused by op 'q_networks/online/dense_1/kernel/read', defined at:\n  File \"C:\\python36\\envs\\machine_learning\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-6a6ed56f5baf>\", line 7, in <module>\n    online_q_values, online_vars = q_network(X_state, name=\"q_networks/online\")\n  File \"<ipython-input-8-511916cb5e00>\", line 15, in q_network\n    kernel_initializer=initializer)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 253, in dense\n    return layer.apply(inputs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 762, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 636, in __call__\n    self.build(input_shapes)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 137, in build\n    trainable=True)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 504, in add_variable\n    partitioner=partitioner)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1262, in get_variable\n    constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1097, in get_variable\n    constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 435, in get_variable\n    constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 404, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 806, in _get_single_variable\n    constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 229, in __init__\n    constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 376, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 127, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2728, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value q_networks/online/dense_1/kernel\n\t [[Node: q_networks/online/dense_1/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@q_networks/online/dense_1/kernel\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](q_networks/online/dense_1/kernel)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value q_networks/online/dense_1/kernel\n\t [[Node: q_networks/online/dense_1/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@q_networks/online/dense_1/kernel\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](q_networks/online/dense_1/kernel)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d8e175ff1053>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0monline_q_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \"\"\"\n\u001b[1;32m--> 648\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4756\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4757\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 4758\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value q_networks/online/dense_1/kernel\n\t [[Node: q_networks/online/dense_1/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@q_networks/online/dense_1/kernel\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](q_networks/online/dense_1/kernel)]]\n\nCaused by op 'q_networks/online/dense_1/kernel/read', defined at:\n  File \"C:\\python36\\envs\\machine_learning\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-6a6ed56f5baf>\", line 7, in <module>\n    online_q_values, online_vars = q_network(X_state, name=\"q_networks/online\")\n  File \"<ipython-input-8-511916cb5e00>\", line 15, in q_network\n    kernel_initializer=initializer)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 253, in dense\n    return layer.apply(inputs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 762, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 636, in __call__\n    self.build(input_shapes)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\layers\\core.py\", line 137, in build\n    trainable=True)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 504, in add_variable\n    partitioner=partitioner)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1262, in get_variable\n    constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1097, in get_variable\n    constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 435, in get_variable\n    constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 404, in _true_getter\n    use_resource=use_resource, constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 806, in _get_single_variable\n    constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 229, in __init__\n    constraint=constraint)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 376, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 127, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 2728, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\python36\\envs\\machine_learning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value q_networks/online/dense_1/kernel\n\t [[Node: q_networks/online/dense_1/kernel/read = Identity[T=DT_FLOAT, _class=[\"loc:@q_networks/online/dense_1/kernel\"], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](q_networks/online/dense_1/kernel)]]\n"
     ]
    }
   ],
   "source": [
    "env = Game2048Env()\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "#     saver.restore(sess, checkpoint_path)\n",
    "    obs = env.reset()\n",
    "    state = preprocess_observation(obs)\n",
    "    while True:\n",
    "        q_values = online_q_values.eval(feed_dict={X_state: [state]})\n",
    "        action = np.argmax(q_values)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        state = preprocess_observation(obs)\n",
    "        print(action, reward)\n",
    "#         env.render()\n",
    "        if done:\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "state = preprocess_observation(obs)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
